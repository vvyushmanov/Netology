1. sparse файлы
   * Разрежённый файл (англ. sparse file) — файл, в котором последовательности нулевых байтов заменены на информацию об этих последовательностях (список дыр). Позволяет избежать фактической записи пустых байтов на диск, вместо этого информация о таких "дырах" хранится в файловой системе. Может использоваться для экономии места и для сжатия файлов. Часто используется для хранения образов дисков виртуальных машин. Минус - потенциальная фрагментация диска, ресурс на работу со списком "дыр"
2. Нет, не могут, так как хардлинки указывают на одну и ту же `inode`, в которой хранится, помимо прочего, информация о нём, в т.ч. права доступа и информация о владельце.  
3. Создание новой ВМ с неразмеченными дисками
```shell 
   vagrant@vagrant:~$ sudo fdisk -l | grep sd
   Disk /dev/sda: 64 GiB, 68719476736 bytes, 134217728 sectors
   /dev/sda1     2048      4095      2048    1M BIOS boot
   /dev/sda2     4096   3149823   3145728  1.5G Linux filesystem
   /dev/sda3  3149824 134215679 131065856 62.5G Linux filesystem
   Disk /dev/sdb: 2.51 GiB, 2684354560 bytes, 5242880 sectors
   Disk /dev/sdc: 2.51 GiB, 2684354560 bytes, 5242880 sectors
```
4. Разбивка диска
   ```shell
   vagrant@vagrant:~$ sudo fdisk /dev/sdb
   <команды в интерактивном режиме>
   vagrant@vagrant:~$ sudo fdisk -l | grep sdb
   Disk /dev/sdb: 2.51 GiB, 2684354560 bytes, 5242880 sectors
   /dev/sdb1          2048 4196351 4194304    2G 83 Linux
   /dev/sdb2       4196352 5242879 1046528  511M 83 Linux
   ```
5. Копирование таблицы разделов
```shell
vagrant@vagrant:~$ sudo sfdisk -d /dev/sdb | sudo sfdisk /dev/sdc
Checking that no-one is using this disk right now ... OK

Disk /dev/sdc: 2.51 GiB, 2684354560 bytes, 5242880 sectors
Disk model: VBOX HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Created a new DOS disklabel with disk identifier 0x372d4770.
/dev/sdc1: Created a new partition 1 of type 'Linux' and of size 2 GiB.
/dev/sdc2: Created a new partition 2 of type 'Linux' and of size 511 MiB.
/dev/sdc3: Done.

New situation:
Disklabel type: dos
Disk identifier: 0x372d4770

Device     Boot   Start     End Sectors  Size Id Type
/dev/sdc1          2048 4196351 4194304    2G 83 Linux
/dev/sdc2       4196352 5242879 1046528  511M 83 Linux

The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.
```

6. Создание RAID1
```shell
vagrant@vagrant:~$ sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: size set to 2094080K
Continue creating array? y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
vagrant@vagrant:~$ cat /proc/mdstat 
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdc1[1] sdb1[0]
      2094080 blocks super 1.2 [2/2] [UU]
      [===================>.]  resync = 95.5% (2001152/2094080) finish=0.0min speed=222350K/sec
      
unused devices: <none>
```
7. Создание RAID0
```shell
vagrant@vagrant:~$ sudo mdadm --create --verbose /dev/md1 --level=0 --raid-devices=2 /dev/sdb2 /dev/sdc2
mdadm: chunk size defaults to 512K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
vagrant@vagrant:~$ cat /proc/mdstat 
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md1 : active raid0 sdc2[1] sdb2[0]
      1042432 blocks super 1.2 512k chunks
      
md0 : active raid1 sdc1[1] sdb1[0]
      2094080 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>
```
8. Создание PV
```shell
vagrant@vagrant:~$ sudo pvcreate /dev/md0
  Physical volume "/dev/md0" successfully created.
vagrant@vagrant:~$ sudo pvcreate /dev/md1
  Physical volume "/dev/md1" successfully created.
vagrant@vagrant:~$ sudo pvs
  PV         VG        Fmt  Attr PSize    PFree   
  /dev/md0             lvm2 ---    <2.00g   <2.00g
  /dev/md1             lvm2 ---  1018.00m 1018.00m
  /dev/sda3  ubuntu-vg lvm2 a--   <62.50g   31.25g
```
9. Создание LVG
```shell
vagrant@vagrant:~$ sudo vgcreate --verbose netology /dev/md0 /dev/md1 
  Wiping signatures on new PV /dev/md0.
  Wiping signatures on new PV /dev/md1.
  Adding physical volume '/dev/md0' to volume group 'netology'
  Adding physical volume '/dev/md1' to volume group 'netology'
  Creating directory "/etc/lvm/archive"
  Archiving volume group "netology" metadata (seqno 0).
  Creating directory "/etc/lvm/backup"
  Creating volume group backup "/etc/lvm/backup/netology" (seqno 1).
  Volume group "netology" successfully created
vagrant@vagrant:~$ sudo pvs
  PV         VG        Fmt  Attr PSize    PFree   
  /dev/md0   netology  lvm2 a--    <2.00g   <2.00g
  /dev/md1   netology  lvm2 a--  1016.00m 1016.00m
  /dev/sda3  ubuntu-vg lvm2 a--   <62.50g   31.25g
```
10. Создайте LV размером 100 Мб, указав его расположение на PV с RAID0.
```shell
vagrant@vagrant:~$ sudo lvcreate --verbose -L 100M -n testlv netology /dev/md1
  Archiving volume group "netology" metadata (seqno 1).
  Creating logical volume testlv
  Creating volume group backup "/etc/lvm/backup/netology" (seqno 2).
  Activating logical volume netology/testlv.
  activation/volume_list configuration setting not defined: Checking only host tags for netology/testlv.
  Creating netology-testlv
  Loading table for netology-testlv (253:1).
  Resuming netology-testlv (253:1).
  Wiping known signatures on logical volume "netology/testlv"
  Initializing 4.00 KiB of logical volume "netology/testlv" with value 0.
  Logical volume "testlv" created.
```
11. Создание файловой системы
```shell
vagrant@vagrant:~$ sudo mkfs.ext4 /dev/netology/testlv 
mke2fs 1.45.5 (07-Jan-2020)
Creating filesystem with 25600 4k blocks and 25600 inodes

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (1024 blocks): done
Writing superblocks and filesystem accounting information: done
```
12. Монтирование (для постоянного монтирования необходимо отредактировать файл /etc/fstab внеся туда строку `/dev/mapper/netology-testlv  /tmp/new/ ext4  errors=remount-ro 0 0` )
```shell
vagrant@vagrant:~$ sudo mount /dev/netology/testlv /tmp/new
vagrant@vagrant:~$ df -Th | grep netology
/dev/mapper/netology-testlv       ext4       93M   72K   86M   1% /tmp/new
```
13. Скачать файл
```shell
vagrant@vagrant:/tmp/new$ sudo wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz
--2022-09-10 11:03:50--  https://mirror.yandex.ru/ubuntu/ls-lR.gz
Resolving mirror.yandex.ru (mirror.yandex.ru)... 213.180.204.183, 2a02:6b8::183
Connecting to mirror.yandex.ru (mirror.yandex.ru)|213.180.204.183|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 22408630 (21M) [application/octet-stream]
Saving to: ‘/tmp/new/test.gz’

/tmp/new/test.gz                  100%[===========================================================>]  21.37M  43.5MB/s    in 0.5s    

2022-09-10 11:03:50 (43.5 MB/s) - ‘/tmp/new/test.gz’ saved [22408630/22408630]
```
14. Вывод lsblk
```shell
vagrant@vagrant:/$ sudo lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
<...>
sdb                         8:16   0  2.5G  0 disk  
├─sdb1                      8:17   0    2G  0 part  
│ └─md0                     9:0    0    2G  0 raid1 
└─sdb2                      8:18   0  511M  0 part  
  └─md1                     9:1    0 1018M  0 raid0 
    └─netology-testlv     253:1    0  100M  0 lvm   
sdc                         8:32   0  2.5G  0 disk  
├─sdc1                      8:33   0    2G  0 part  
│ └─md0                     9:0    0    2G  0 raid1 
└─sdc2                      8:34   0  511M  0 part  
  └─md1                     9:1    0 1018M  0 raid0 
    └─netology-testlv     253:1    0  100M  0 lvm
```
15. Тест
```shell
vagrant@vagrant:/tmp/new$ gzip -t /tmp/new/test.gz
vagrant@vagrant:/tmp/new$ echo $?
0
```
16. Перемещение PV
```shell
vagrant@vagrant:~$ sudo pvmove --verbose /dev/md1 /dev/md0
  Archiving volume group "netology" metadata (seqno 2).
  Creating logical volume pvmove0
  activation/volume_list configuration setting not defined: Checking only host tags for netology/testlv.
  Moving 25 extents of logical volume netology/testlv.
  activation/volume_list configuration setting not defined: Checking only host tags for netology/testlv.
  Creating netology-pvmove0
  Loading table for netology-pvmove0 (253:2).
  Loading table for netology-testlv (253:1).
  Suspending netology-testlv (253:1) with device flush
  Resuming netology-pvmove0 (253:2).
  Resuming netology-testlv (253:1).
  Creating volume group backup "/etc/lvm/backup/netology" (seqno 3).
  activation/volume_list configuration setting not defined: Checking only host tags for netology/pvmove0.
  Checking progress before waiting every 15 seconds.
  /dev/md1: Moved: 20.00%
  /dev/md1: Moved: 100.00%
  Polling finished successfully.
```
17. Выставить --fail одному из устройств в RAID1
```shell
vagrant@vagrant:/tmp/new$ sudo mdadm --fail /dev/md0 /dev/sdc1 
mdadm: set /dev/sdc1 faulty in /dev/md0
```
18. Вывод `dmesg`
```shell
[ 7292.591945] md/raid1:md0: Disk failure on sdc1, disabling device.
               md/raid1:md0: Operation continuing on 1 devices.

```
19. Повторный тест
```shell
vagrant@vagrant:~$ gzip -t /tmp/new/test.gz
vagrant@vagrant:~$ echo $?
0
```